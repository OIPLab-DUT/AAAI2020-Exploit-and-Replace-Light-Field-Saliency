import torchfrom torch.autograd import Variablefrom torch.utils.data import DataLoaderimport torchvisionimport torch.nn.functional as Fimport torch.optim as optimfrom dataset_loader import MyData, MyTestData, DTestDatafrom model import FocalNet, FocalNet_subfrom conv_lstm import ConvLSTMfrom functions import imsaveimport argparsefrom Trainer_Teacher import Trainerimport osif __name__ == '__main__':    configurations = {        1: dict(            max_iteration=500000,            lr=1.0e-10,            momentum=0.99,            weight_decay=0.0005,            spshot=10000,            nclass=2,            sshow=10,            focal_num=12,        )    }    parser=argparse.ArgumentParser()    parser.add_argument('--phase', type=str, default='test', help='train or test')    parser.add_argument('--param', type=str, default=True, help='path to pre-trained parameters')    parser.add_argument('--train_dataroot', type=str, default='H:\Light-Field\\train_data', help='path to train data')    parser.add_argument('--test_dataroot', type=str, default='H:\Light-Field\\test_data', help='path to test data')    parser.add_argument('--snapshot_root', type=str, default='./snapshot_Teacher', help='path to snapshot')    parser.add_argument('--salmap_Teacherroot', type=str, default='./sal_Teacher', help='path to Teacher saliency map')    parser.add_argument('-c', '--config', type=int, default=1, choices=configurations.keys())    args = parser.parse_args()    cfg = configurations[args.config]    cuda = torch.cuda.is_available    """""""""""~~~ dataset loader ~~~"""""""""    train_dataRoot = args.train_dataroot    test_dataRoot = args.test_dataroot    if not os.path.exists(args.snapshot_root):        os.mkdir(args.snapshot_root)    if not os.path.exists(args.salmap_Teacherroot):        os.mkdir(args.salmap_Teacherroot)    if args.phase == 'train':        SnapRoot = args.snapshot_root           # checkpoint        train_loader = torch.utils.data.DataLoader(MyData(train_dataRoot, transform=True),                                                   batch_size=1, shuffle=False, num_workers=4, pin_memory=True)    else:        MapTeacherRoot = args.salmap_Teacherroot        test_loader = torch.utils.data.DataLoader(MyTestData(test_dataRoot, transform=True),                                       batch_size=1, shuffle=True, num_workers=4, pin_memory=True)    print ('data already')    """"""""""" ~~~nets~~~ """""""""    start_epoch = 0    start_iteration = 0    model_focal = FocalNet(cfg['nclass'],refine=True)    model_focal_sub = FocalNet_sub(cfg['nclass'])    model_clstm = ConvLSTM(input_channels=64, hidden_channels=[64, 32, 64],                     kernel_size=5, step=4, effective_step=[2, 4, 8])    if args.param is True:        model_focal.load_state_dict(torch.load(os.path.join('./snapshot_Teacher', 'focal_snapshot_iter_500000.pth')))        model_clstm.load_state_dict(torch.load(os.path.join('./snapshot_Teacher', 'clstm_snapshot_iter_500000.pth')))        model_focal_sub.load_state_dict(torch.load(os.path.join('./snapshot_Teacher', 'focal_sub_snapshot_iter_500000.pth')))    else:        vgg19_bn = torchvision.models.vgg19_bn(pretrained=True)        model_focal.copy_params_from_vgg19_bn_focal(vgg19_bn)    if cuda:       model_focal = model_focal.cuda()       model_focal_sub = model_focal_sub.cuda()       model_clstm = model_clstm.cuda()    if args.phase == 'train':        # Trainer: class, defined in trainer.py        optimizer_focal = optim.SGD(model_focal.parameters(), lr=cfg['lr'],momentum=cfg['momentum'], weight_decay=cfg['weight_decay'])        optimizer_focal_sub = optim.SGD(model_focal_sub.parameters(), lr=cfg['lr'],momentum=cfg['momentum'], weight_decay=cfg['weight_decay'])        optimizer_clstm = optim.SGD(model_clstm.parameters(), lr=cfg['lr'],momentum=cfg['momentum'], weight_decay=cfg['weight_decay'])        training = Trainer(            cuda=cuda,            model_focal=model_focal,            model_focal_sub = model_focal_sub,            model_clstm=model_clstm,            optimizer_focal=optimizer_focal,            optimizer_focal_sub = optimizer_focal_sub,            optimizer_clstm=optimizer_clstm,            train_loader= train_loader,            max_iter=cfg['max_iteration'],            snapshot=cfg['spshot'],            outpath=args.snapshot_root,            sshow=cfg['sshow']        )        training.epoch = start_epoch        training.iteration = start_iteration        training.train()    else:        for id, (data, focal, img_name, img_size) in enumerate(test_loader):            print('testing bach %d' % id)            inputs = Variable(data).cuda()            inputs_focal = Variable(focal).cuda()            basize, dime, height, width = inputs_focal.size()            inputs_focal = inputs_focal.view(1,basize,dime,height,width).transpose(0,1)            inputs_focal = torch.cat(torch.chunk(inputs_focal, 12, dim=2), dim=1)            inputs_focal = torch.cat(torch.chunk(inputs_focal, basize, dim=0), dim=1).squeeze()            score_focal = model_focal(inputs_focal)            score_pre = model_focal_sub(score_focal)            score_pred1, score_pred2, score_pred3, score_pred4, score_pred5, score_pred6, score_pred7, score_pred8, score_pred9, score_pred10, score_pred11, score_pred12 = torch.chunk(                score_pre, 12, dim=0)            outputs_Teacher = (score_pred1 + score_pred2+score_pred3+ score_pred4+ score_pred5+ score_pred6+ score_pred7+ score_pred8+ score_pred9+  score_pred10+  score_pred11+  score_pred12) /12            outputs_Teacher = model_clstm(score_focal)            outputs_Teacher = F.softmax(outputs_Teacher, dim=1)            outputs = outputs_Teacher[0][1]            outputs = outputs.cpu().data.resize_(height, width)            imsave(os.path.join(MapTeacherRoot, img_name[0] + '.png'), outputs, img_size)        torch.cuda.empty_cache()